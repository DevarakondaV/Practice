{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Linear Regression Models\n",
    "#Chapter 3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Temp Functions to be deleted\n",
    "#plots heat map of multi_gauss\n",
    "def plot_multi_gauss(x,y,mu,covar):\n",
    "    nbins = 50\n",
    "    xx,yy = np.mgrid[x.min():x.max():nbins*1j,y.min():y.max():nbins*1j]\n",
    "    xxyy = np.vstack([xx.flatten(), yy.flatten()]).T\n",
    "    zz = multi_gauss(xxyy,mu,covar)\n",
    "    plt.pcolormesh(xx,yy,zz.reshape(xx.shape))\n",
    "    plt.show()\n",
    "    \n",
    "#multi gauss\n",
    "def multi_gauss(x,mu,cov):\n",
    "    D = len(mu)\n",
    "    a = 1.0/(np.power(2*np.pi,D/2.0))\n",
    "\n",
    "    b = 1.0/(np.power(np.linalg.det(cov),.5))\n",
    "\n",
    "    c = np.exp((-.5)*(maha_dist(x,mu,cov)))\n",
    "    return a*b*c\n",
    "\n",
    "#mahalanobis dist\n",
    "def maha_dist(x,mu,cov):\n",
    "    diff = x-mu\n",
    "    ret = np.matmul(diff,np.linalg.inv(cov))\n",
    "    ret = np.matmul(ret,diff.T)\n",
    "    return ret.diagonal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sin(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "def cos(x):\n",
    "    return np.cos(x)\n",
    "\n",
    "def pw(x,n):\n",
    "    return np.power(x,n)\n",
    "\n",
    "def gauss_sample(m,s):\n",
    "    return np.random.normal(m,s)\n",
    "\n",
    "def Y(x,m,b):\n",
    "    return (m*x)+b\n",
    "\n",
    "def nlin_fun(x):\n",
    "    y = (10*sin(x))+Y(x,0.2,-.5)-(3*cos(2*x))\n",
    "    return y\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_basis(x,mu,s):\n",
    "    N = len(mu)\n",
    "    x = np.outer(np.transpose(x),np.ones(N))\n",
    "    diff = np.subtract(x,mu)\n",
    "    return sigmoid((1.0/s)*diff)\n",
    "\n",
    "def gauss_basis(x,mu,s):\n",
    "    N = len(mu)\n",
    "    x = np.outer(np.transpose(x),np.ones(N))\n",
    "    diff = np.subtract(x,mu)\n",
    "    n = np.power(diff,2)\n",
    "    d = 2*np.power(s,2)\n",
    "    f = (1.0/d)*n\n",
    "    return np.exp(-f)\n",
    "\n",
    "def poly_basis(x,deg):\n",
    "    x = np.outer(np.transpose(x),np.ones(deg))\n",
    "    return np.power(x,deg)\n",
    "\n",
    "#returns wegihts based on zero mean gaussian\n",
    "def get_weights(numb):\n",
    "    return np.random.normal(0,.5,numb)\n",
    "\n",
    "def get_centers(x,n_bfun):\n",
    "    if len(x) == 1:\n",
    "        c = np.random.normal(x[0],.3,n_bfun)\n",
    "        return c\n",
    "    if n_bfun == 0:\n",
    "        return []\n",
    "    if n_bfun == 1:\n",
    "        return [np.average(x)]\n",
    "    mi = min(x)\n",
    "    ma = max(x)\n",
    "    delt = (ma-mi)/n_bfun\n",
    "    cen = []\n",
    "    while mi<ma and len(cen)<n_bfun:\n",
    "        cen.append(mi)\n",
    "        mi = mi+delt\n",
    "    \n",
    "    return cen\n",
    "\n",
    "def moore_pen_inv(phi):\n",
    "    m = np.matmul(np.transpose(phi),phi)\n",
    "    m = np.matmul(np.linalg.inv(m),np.transpose(phi))\n",
    "    return m\n",
    "\n",
    "#The approxiamte function y(x,w) given input x\n",
    "#weights and basis functions\n",
    "def approx_y(w,x,c,n_bf,s,p):\n",
    "    b,c = pick_basis(x,n_bf-1,s,p)\n",
    "    g = np.ones(len(x))\n",
    "    g = np.insert(b,0,g,1)\n",
    "    return np.matmul(w,np.transpose(g))\n",
    "\n",
    "\n",
    "#Returns maximum likelihood weights and basis functions\n",
    "#for linear regression\n",
    "#pts is the input points\n",
    "#n_bf is the number of basis functions\n",
    "#s is the spatial component of the basis\n",
    "#v is the type of basis function 1->gauss 2->sigmoid\n",
    "def max_likelihood_lin_reg(pts,n_bf,s,p):\n",
    "    x = pts[:,0]\n",
    "    y = pts[:,1]\n",
    "    \n",
    "    g = np.ones(len(x))\n",
    "    bf,c = pick_basis(x,n_bf-1,s,p)\n",
    "    g = np.insert(bf,0,g,1)\n",
    "    w = np.matmul(moore_pen_inv(g),y)\n",
    "    return w,g,c\n",
    "\n",
    "#Returns maximum likelihood weights and basis functions\n",
    "#for linear regression with regularization term\n",
    "#pts is the input points\n",
    "#n_bf is the number of basis functions\n",
    "#s is the spatial component of the basis\n",
    "#p is the type of basis function 1->gauss 2->sigmoid\n",
    "#rg_fac is the regularization factor \n",
    "def max_likelihood_lin_reg_rg(pts,n_bf,s,p,rg_fac):\n",
    "    x = pts[:,0]\n",
    "    y = pts[:,1]\n",
    "    \n",
    "    #c = get_centers(x,n_bf-1)\n",
    "    g = np.ones(len(x))\n",
    "    bf,c = pick_basis(x,n_bf-1,s,p)\n",
    "    g = np.insert(bf,0,g,1)\n",
    "    shp = np.shape(g)\n",
    "    rg_I = rg_fac*np.eye(shp[1],shp[1],dtype=float)\n",
    "    \n",
    "    #calculating weight vector\n",
    "    a = rg_I+np.matmul(np.transpose(g),g)\n",
    "    b = np.matmul(np.linalg.inv(a),np.transpose(g))\n",
    "    w = np.matmul(b,y) \n",
    "    return w,g,c\n",
    "\n",
    "#Returns basis func\n",
    "def pick_basis(data,n_bf,s,p):\n",
    "    if ((p==1) or (p==2)):\n",
    "        c = get_centers(data,n_bf)\n",
    "        \n",
    "    if (p == 1):\n",
    "        return gauss_basis(data,c,s),c\n",
    "    elif (p==2):\n",
    "        return sigmoid_basis(data,c,s),c\n",
    "    elif (p==3):\n",
    "        return poly_basis(data,n_bf),0\n",
    "    \n",
    "#Parameter Distribution Posterior Estimation\n",
    "#p(w|t,m_0,s_0)\n",
    "def weight_dist_post(data,prior_mean,prior_covariance,noise,n_bf,s,p):\n",
    "    \n",
    "    x = data[:,0]\n",
    "    y = data[:,1]\n",
    "    phi,c = pick_basis(x,n_bf-1,s,p)\n",
    "    g = np.ones(len(x))\n",
    "    phi = np.insert(phi,0,g,1)\n",
    "    \n",
    "    a = noise*np.matmul(np.transpose(phi),phi)\n",
    "    post_cov_inv = np.linalg.inv(prior_covariance)+a\n",
    "    \n",
    "    a = noise*np.matmul(np.transpose(phi),y)\n",
    "    a = np.matmul(np.linalg.inv(prior_covariance),prior_mean)+a\n",
    "    post_mean = np.matmul(np.linalg.inv(post_cov_inv),a)\n",
    "    \n",
    "    return post_mean,np.linalg.inv(post_cov_inv),c\n",
    "\n",
    "#Linear regression on linear function defined by paramteres\n",
    "#m and b\n",
    "def post_weights_linear_example(m,b):\n",
    "    x = np.arange(0,3,.002)\n",
    "    print(\"Len x\",len(x))\n",
    "    print(\"Min x\",min(x))\n",
    "    print(\"Max x\",max(x))\n",
    "    \n",
    "    np.random.shuffle(x)\n",
    "    y = gauss_sample(Y(x,m,b),.3)\n",
    "    pts = np.stack((x,y),axis=1)\n",
    "    \n",
    "    n = 0\n",
    "    s = .5\n",
    "    n_bf = 2\n",
    "    p = 3\n",
    "    m = np.array([1,1])\n",
    "    cov = np.array([[1,3.0/5],[3.0/5,2]])\n",
    "    cen = []\n",
    "    len_x = len(x)\n",
    "    print(len_x)\n",
    "\n",
    "    wx = np.arange(m[0]-2*(.1),(m[0]+2*(.1)),.01)\n",
    "    wy = np.arange(m[1]-2*(.1),(m[1]+2*(.1)),.01)\n",
    "    plot_multi_gauss(wx,wy,m,cov)\n",
    "\n",
    "    while n<(len_x):\n",
    "        m,cov,c = weight_dist_post(pts[n:n+5],m,cov,25,n_bf,s,p)\n",
    "        n = n+5\n",
    "\n",
    "    \n",
    "    ww,gg,cc = max_likelihood_lin_reg(pts,8,s,1)\n",
    "    yy = approx_y(ww,x,cc,8,s,1)\n",
    "    \n",
    "    \n",
    "    print(\"Weights post:\", m)\n",
    "    print(\"Covariance\", cov)\n",
    "    wx = np.arange(m[0]-2*(.1),(m[0]+2*(.1)),.01)\n",
    "    wy = np.arange(m[1]-2*(.1),(m[1]+2*(.1)),.01)\n",
    "    plot_multi_gauss(wx,wy,m,cov)\n",
    "\n",
    "    #def approx_y(w,x,c,s,p):\n",
    "    yyy = Y(x,m[1],m[0])\n",
    "    plt.plot(x,y,'.')\n",
    "    plt.plot(x,yy,'.')\n",
    "    plt.plot(x,yyy,'.')\n",
    "    plt.legend([\"True\",\"Estimate_ML\",\"Estiamte_Post\"])\n",
    "    plt.show()\n",
    "    \n",
    "#Posterior weights distribution non linear function\n",
    "#Example\n",
    "def post_weights_nonlin_example():\n",
    "    x = np.arange(0,2*np.pi,.02)\n",
    "    print(\"Len x\",len(x))\n",
    "    print(\"min x\",min(x))\n",
    "    print(\"max x\",max(x))\n",
    "\n",
    "    np.random.shuffle(x)\n",
    "    y = gauss_sample((10*sin(x))+Y(x,0.2,-.5)-(3*cos(2*x)),.3)\n",
    "    pts = np.stack((x,y),axis=1)\n",
    "\n",
    "\n",
    "    n = 0\n",
    "    s = .5\n",
    "    n_bf = 10\n",
    "    p = 1\n",
    "    m = np.array([1,1,1,1,1,1,1,1,1,1])\n",
    "    cov = np.random.rand(10,10)\n",
    "    cen = []\n",
    "    len_x = len(x)\n",
    "\n",
    "    while n<(len_x):\n",
    "        m,cov,c = weight_dist_post(pts[n:n+20],m,cov,25,n_bf,s,p)\n",
    "        cen.append(c)\n",
    "        n = n+20\n",
    "\n",
    "    \n",
    "    ww,gg,cc = max_likelihood_lin_reg(pts,8,s,1)\n",
    "    yy = approx_y(ww,x,cc,8,s,1)\n",
    "\n",
    "    print(\"Weight Estimates\",  m)\n",
    "\n",
    "    yyy = approx_y(m,x,cen,10,s,1)\n",
    "    plt.plot(x,y,'.')\n",
    "    plt.plot(x,yy,'.')\n",
    "    plt.plot(x,yyy,'.')\n",
    "    plt.legend([\"True\",\"Estimate_ML\",\"Estiamte_Post\"])\n",
    "    plt.show()\n",
    "\n",
    "#Equivalent Kernel Approximation\n",
    "def equivalent_kernel(x,xn,n_bf,cov,noise,s,p):\n",
    "    phi_x,c = pick_basis(x,n_bf-1,s,p)\n",
    "    phi_xn,c = pick_basis(xn,n_bf-1,s,p)\n",
    "    g_x = np.ones(len(x))\n",
    "    g_xn = np.ones(len(xn))\n",
    "    phi_x = np.insert(phi_x,0,g_x,1)\n",
    "    phi_xn = np.insert(phi_xn,0,g_xn,1)\n",
    "    \n",
    "    \n",
    "    a = noise*phi_x\n",
    "    a = np.matmul(a,cov)\n",
    "    a = np.matmul(a,np.transpose(phi_xn))\n",
    "    return a\n",
    "\n",
    "#Evidence Approximation for hyper parameters\n",
    "#p(alpha,beta|t)\n",
    "\n",
    "\n",
    "\n",
    "#Performs a kernel estimation example on a non linear function\n",
    "def kernel_example():\n",
    "    x = np.arange(-2*np.pi,2*np.pi,.01)\n",
    "    np.random.shuffle(x)\n",
    "    y = gauss_sample(nlin_fun(x),1)\n",
    "    #y = gauss_sample(Y(x,5,.3),1)\n",
    "    pts = np.stack((x,y),axis=1)\n",
    "    \n",
    "    n = 0\n",
    "    s = 1\n",
    "    n_bf = 10\n",
    "    m = np.zeros(10)\n",
    "    cov = np.random.rand(10,10)\n",
    "    len_x = len(x)\n",
    "    p=1\n",
    "    \n",
    "    while n<(len_x):\n",
    "        m,cov,c = weight_dist_post(pts[n:n+20],m,cov,25,n_bf,s,p)\n",
    "        n = n+20\n",
    "    \n",
    "    xx = np.arange(-2*np.pi,2*np.pi,.1)\n",
    "    K = equivalent_kernel(xx,x,n_bf,cov,25,s,p)\n",
    "    yy = np.matmul(K,y)\n",
    "    print(np.shape(y))\n",
    "    yyy = approx_y(m,x,0,10,s,p)\n",
    "    plt.plot(x,y,'.')\n",
    "    plt.plot(xx,nlin_fun(xx),'x')\n",
    "    plt.plot(xx,yy,'x')\n",
    "    plt.plot(x,yyy,'.')\n",
    "    plt.legend([\"Train data\",\"True Point\",\"Kernel-Estimate\",\"Weight Estimate\"])\n",
    "    plt.show()\n",
    "    return m,cov,K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.333280389861\n",
      "0.00163030309626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def approx_alpha_beta(x,y,n_bf,s,p,alpha,beta):\n",
    "    phi,c = pick_basis(x,n_bf-1,s,p)\n",
    "    g = np.ones(len(x))\n",
    "    phi = np.insert(phi,0,g,1)\n",
    "    \n",
    "    new_alpha = alpha\n",
    "    new_beta = beta\n",
    "    \n",
    "    diff_a = 5\n",
    "    diff_b = 5\n",
    "    while(diff_a>.01 or diff_b > .01):\n",
    "        a = np.matmul(np.transpose(phi),phi)\n",
    "        A = new_alpha*np.eye(np.shape(a)[0])+(new_beta*a)\n",
    "        \n",
    "        \n",
    "        mn = new_beta*np.matmul(np.linalg.inv(A),np.matmul(np.transpose(phi),y))\n",
    "        e_vals,v = np.linalg.eig(new_beta*a)\n",
    "        gamma = np.sum(1.0/(e_vals+new_alpha))\n",
    "        alpha = gamma/(np.matmul(np.transpose(mn),mn))\n",
    "        diff = y-np.matmul(mn,np.transpose(phi))\n",
    "        sq_sum = (1.0/(len(x)-gamma))*np.sum(np.power(diff,2))\n",
    "        beta = 1.0/sq_sum\n",
    "        \n",
    "        diff_a = np.abs(alpha-new_alpha)\n",
    "        diff_b = np.abs(beta-new_beta)\n",
    "    \n",
    "        new_alpha = alpha\n",
    "        new_beta = beta\n",
    "    \n",
    "    return alpha,beta\n",
    "\n",
    "\n",
    "#Returns the posterior weight distribution\n",
    "#Given new data x and y\n",
    "def weight_dist_zero_mean_prior(pts,prior_m,prior_cov,alpha,beta,n_bf,s,p):\n",
    "    x=  pts[:,0]\n",
    "    y = pts[:,1]\n",
    "    \n",
    "    phi,c = pick_basis(x,n_bf-1,s,p)\n",
    "    g = np.ones(len(x))\n",
    "    phi = np.insert(phi,0,g,1)\n",
    "    \n",
    "    a = beta*np.matmul(np.transpose(phi),phi)\n",
    "    post_cov_inv = alpha*np.eye(np.shape(a)[0])+a\n",
    "    post_m = beta*np.matmul(np.linalg.inv(post_cov_inv),np.matmul(np.transpose(phi),y))\n",
    "    return post_m,np.linalg.inv(post_cov_inv)\n",
    "\n",
    "\n",
    "def evidence_approximation(pts,alpha,beta,n_bf,s,p):\n",
    "    x = pts[:,0]\n",
    "    y = pts[:,1]\n",
    "    N = len(x)\n",
    "    M = len(y)\n",
    "    \n",
    "    phi,c = pick_basis(x,n_bf-1,s,p)\n",
    "    g = np.ones(len(x))\n",
    "    phi = np.insert(phi,0,g,1)\n",
    "    \n",
    "    a = beta*np.matmul(np.transpose(phi),phi)\n",
    "    A = alpha*np.eye(np.shape(a)[0])+a\n",
    "    mn = beta*np.matmul(np.linalg.inv(A),np.matmul(np.transpose(phi),y))\n",
    "\n",
    "    diff = y-np.matmul(phi,mn)\n",
    "    Emn = (beta/2.0)*np.power(np.absolute(diff),2)+((alpha/2.0)*np.matmul(np.transpose(mn),mn))\n",
    "    \n",
    "    AA = np.power(2*np.pi,M/2.0)*np.power(np.linalg.norm(A),-.5)\n",
    "    it = np.exp(-1*Emn)*AA\n",
    "    bt = np.power(beta/(2*np.pi),N/2)\n",
    "    print(beta)\n",
    "    print(alpha)\n",
    "    E = np.power(beta/(2*np.pi),N/2.0)*np.power(alpha/(2*np.pi),M/2.0)*it\n",
    "    #return E\n",
    "    return E\n",
    "\n",
    "    \n",
    "    \n",
    "x = np.arange(0,2*np.pi,.01)\n",
    "np.random.shuffle(x)\n",
    "y = gauss_sample(nlin_fun(x),1)\n",
    "pts = np.stack((x,y),axis=1)\n",
    "#def evidence_approx(x,y,n_bf,s,p,alpha,beta):\n",
    "a = .3\n",
    "b = .3\n",
    "p = 1\n",
    "s = .5\n",
    "n_bf = np.arange(1,11,1)\n",
    "m =[[np.zeros(1),np.zeros(2),np.zeros(3),\n",
    "               np.zeros(4),np.zeros(5),np.zeros(6),\n",
    "               np.zeros(7),np.zeros(8),np.zeros(9),\n",
    "               np.zeros(10)]]\n",
    "#cov = np.random.rand(10,10)\n",
    "\n",
    "\n",
    "#a,b = evidence_approx(x,y,n_bf,s,p,a,b)\n",
    "ab = []\n",
    "res = []\n",
    "for i in range(0,10):\n",
    "    a = .3\n",
    "    b = .3\n",
    "    a,b = approx_alpha_beta(x,y,n_bf[i],s,p,a,b)\n",
    "    ab.append([a,b])\n",
    "    cov = np.random.rand(n_bf[i],n_bf[i])\n",
    "    n =0\n",
    "    len_x = len(x)\n",
    "    while n<(len_x):\n",
    "        mm,cov = weight_dist_zero_mean_prior(pts[n:n+20],m[0][i],cov,a,b,n_bf[i],s,p)\n",
    "        n = n+20\n",
    "    res.append([mm,cov])\n",
    "\n",
    "\n",
    "#evidence_approximation(pts,m,alpha,beta,n_bf,s,p)\n",
    "evidence_approximation(pts,ab[5][0],ab[5][1],n_bf[5],s,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
